{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4608a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch_geometric\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.loader.dataloader import DataLoader\n",
    "\n",
    "from GraphDataset import RandomGraphDataset\n",
    "from GraphToSequence import graphToSequence, sequenceToGraph\n",
    "\n",
    "from SimpleTransformer import SimpleTransformer, Transformer\n",
    "from lang import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5255a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_training = \"/eos/user/c/czeh/graphsequencer/fixed_edge_graph_training\"\n",
    "data_folder_test = \"/eos/user/c/czeh/graphsequencer/fixed_edge_graph_test\"\n",
    "\n",
    "\n",
    "dataset_training = RandomGraphDataset(data_folder_training, nodes=5, edges=3, data_count=100)\n",
    "dataset_test = RandomGraphDataset(data_folder_test, nodes=5,  edges=3, data_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "984fff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(dataset_training, shuffle=True)\n",
    "test_dl = DataLoader(dataset_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4a94d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 10\n",
    "max_seq_length = 10\n",
    "max_edge_count = 6\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a444fa64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d8cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0d753d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b54a1f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating data\n",
    "num_samples = 1000\n",
    "\n",
    "converter = Lang(dataset_training.get(0).num_nodes)\n",
    "vocab_size = converter.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc6f538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "# model = SimpleTransformer(embed_size, input_length, vocab_size).to(device)\n",
    "model = Transformer(dataset_training.get(0).num_nodes, vocab_size, d_model, num_heads, num_layers, d_ff, max_edge_count, max_seq_length, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Optionally introduce weight decay\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f4e9a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 100/100 [00:06<00:00, 15.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.117625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 100/100 [00:06<00:00, 15.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.738416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 100/100 [00:05<00:00, 16.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.660407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 100/100 [00:05<00:00, 18.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.572286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 100/100 [00:05<00:00, 17.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.499651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 100/100 [00:06<00:00, 16.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.417463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 100/100 [00:06<00:00, 16.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.368894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 100/100 [00:05<00:00, 17.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.306604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 100/100 [00:05<00:00, 17.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.294084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 100/100 [00:05<00:00, 17.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.257906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 100/100 [00:06<00:00, 16.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 0.246845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 100/100 [00:06<00:00, 16.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 0.267315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  48%|████▊     | 48/100 [00:02<00:03, 16.68it/s]"
     ]
    }
   ],
   "source": [
    "losses_per_epoch = []\n",
    "\n",
    "# Optionally introduce gradient clipping\n",
    "# torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "    for sample in tqdm(train_dl, desc=\"training\"):\n",
    "        sample_seq = sample[0].y\n",
    "        \n",
    "        input_tensor = torch.zeros((len(sample_seq)+1, input_length), dtype=torch.long).to(device)\n",
    "        target_tensor = torch.zeros((len(sample_seq)+1, input_length), dtype=torch.long).to(device)\n",
    "        edge_tensor = torch.zeros((len(sample_seq)+1, max_edge_count), dtype=torch.long).to(device)\n",
    "        for i in range(len(sample_seq)+1):\n",
    "            input_seq = converter.subseq2arr(sample_seq, input_length+1, max(i-input_length-1, 0), min(i+1, input_length+1))\n",
    "            input_tensor[i, :] = torch.tensor(input_seq[:-1], dtype=torch.long)\n",
    "            target_tensor[i, :] = torch.tensor(input_seq[1:], dtype=torch.long)\n",
    "            edge_tensor[i, :] = torch.flatten(sample.edge_index.T).unsqueeze(0)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        output = model(edge_tensor, input_tensor)\n",
    "        loss = criterion(output.contiguous().view(-1, vocab_size), target_tensor.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    loss = total_loss / len(train_dl)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.6f}\")\n",
    "    losses_per_epoch.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222f61a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_per_epoch)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0fd1dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "model.eval() \n",
    "torch.no_grad()\n",
    "for sample in tqdm(test_dl, desc=\"test\"):\n",
    "    \n",
    "    input_tensor = torch.tensor(converter.subseq2arr(sample.y, input_length, 0, 0)).unsqueeze(0).to(device)\n",
    "    edge_tensor = torch.flatten(sample.edge_index.T).unsqueeze(0).to(device)\n",
    "    \n",
    "    predictions = model(edge_tensor, input_tensor)\n",
    "    predicted_index = predictions.argmax(-1)\n",
    "    predicted_number = predicted_index[0, -1].item()\n",
    "\n",
    "    step = 0\n",
    "    while (predicted_number != 2 and step < 15):  # Disable gradient computation for inference\n",
    "        input_tensor = torch.roll(input_tensor, -1, dims=1)\n",
    "        input_tensor[0, -1] = predicted_number\n",
    "    \n",
    "        predictions = model(edge_tensor, input_tensor)\n",
    "        predicted_index = predictions.argmax(-1)  # Get the index of the max log-probability for the last position\n",
    "        predicted_number = predicted_index[0, -1].item()  # Convert to Python number\n",
    "        step += 1\n",
    "     \n",
    "    input_tensor = torch.roll(input_tensor, -1, dims=1)\n",
    "    input_tensor[0, -1] = predicted_number \n",
    "    check = converter.arr2seq(input_tensor[0]) == converter.arr2seq(converter.subseq2arr(sample[0].y, input_length, 0, input_length))\n",
    "    \n",
    "    print(f\"Input Sequence: {sample[0].y}\")\n",
    "    print(f\"Predicted Sequence: {converter.arr2seq(input_tensor[0])}\")\n",
    "    print(f\"Correct: {check}\")\n",
    "    \n",
    "    correct += check\n",
    "\n",
    "print(f\"Percentage of correct guesses: {correct / (len(test_dl))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8d4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae4fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ff108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
